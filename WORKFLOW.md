# Agentic Workflow Architecture

A detailed technical walkthrough of the multi-agent essay writer's orchestration system, state management, and intelligent routing.

---

## Table of Contents

1. [Overview](#overview)
2. [Agent Roles & Responsibilities](#agent-roles--responsibilities)
3. [State Management](#state-management)
4. [Workflow Phases](#workflow-phases)
5. [Routing Logic](#routing-logic)
6. [Node History Tracking](#node-history-tracking)
7. [State Transitions](#state-transitions)
8. [Code Deep Dive](#code-deep-dive)

---

## Overview

This essay writer implements an **editor-orchestrated multi-agent system** where specialized agents collaborate through iterative feedback loops. Unlike simple sequential pipelines, this architecture features:

- **Dynamic routing** based on essay quality and editorial decisions
- **Stateful coordination** through comprehensive state tracking
- **Adaptive workflows** that respond to critique and research needs
- **Intelligent decision-making** by the editor agent

### Key Innovation: Editorial Orchestration

The **Editor** doesn't just create outlines—it acts as an **intelligent coordinator** that:
1. Reviews critic feedback after each draft
2. Decides whether to commission more research, direct revisions, or approve
3. Tracks workflow progress and enforces iteration limits
4. Provides specific direction to the writer for revisions

This creates a **self-improving system** that iterates until quality standards are met.

---

## Agent Roles & Responsibilities

### 1. Editor (Orchestrator)

**Primary Role**: Strategic decision-maker and workflow coordinator

**Responsibilities**:
- Develop thesis statement and essay outline
- Commission research by generating search queries
- Review critic feedback and make routing decisions
- Provide specific direction to writer for revisions
- Decide when essay quality is sufficient

**Decision Types**:
- `"research"` - Commission additional research to strengthen arguments
- `"pass_to_writer"` - Pass newly acquired research to writer
- `"revise"` - Direct writer to revise with specific guidance
- `"approve"` - Essay meets quality standards, workflow complete

**Key State Updates**:
```python
{
    "thesis": "...",                    # Core argument
    "outline": "...",                   # Essay structure
    "research_queries": ["...", "..."], # Search queries for researcher
    "editor_direction": "...",          # Specific guidance for writer
    "editor_decision": "research",      # Routing decision
    "editing_complete": True,           # Outline phase done
    "essay_complete": True              # Final approval
}
```

**Model Recommendation**: Most capable model (e.g., GPT-5.1) for strategic thinking

---

### 2. Researcher

**Primary Role**: Web research and information synthesis

**Responsibilities**:
- Execute search queries generated by editor
- Gather web content using Tavily API
- Synthesize and summarize research findings
- Return consolidated research to editor

**Token Usage**: ~50,000 tokens per research cycle (summarizing multiple web sources)

**Configuration**:
- Uses `max_queries` from state to limit number of queries
- Uses `max_results_per_query` from state to control search depth
- Helps control token usage and costs

**Key State Updates**:
```python
{
    "research_results": [{"query": "...", "results": [...]}]  # Appends new research with query context
}
```

**Model Recommendation**: Cheapest capable model (e.g., GPT-5 Nano) to minimize cost

**Why Cost Matters**: Research involves processing large volumes of text from web sources. The researcher summarizes this into digestible insights. Since this is primarily summarization (not complex reasoning), using cheap models here can save 10-20x on costs with minimal quality impact.

---

### 3. Writer

**Primary Role**: Draft creation and revision

**Responsibilities**:
- Generate initial essay draft from outline and research
- Revise draft based on editor direction
- Follow specific guidance provided by editor
- Maintain essay structure and target length

**Key State Updates**:
```python
{
    "draft": "...",              # Current essay text
    "writing_iteration": 2       # Iteration counter
}
```

**Model Recommendation**: Balanced model (e.g., GPT-5 Mini) for quality writing at reasonable cost

**Routing Context**:
- Writer is invoked when `editing_complete=True` (initial draft)
- Writer is invoked when `editor_decision="revise"` (revision)
- Writer is invoked when `editor_decision="pass_to_writer"` (new research available)

---

### 4. Critic

**Primary Role**: Quality evaluation and feedback

**Responsibilities**:
- Evaluate draft against quality criteria
- Provide specific, actionable feedback
- Identify weaknesses in argumentation, structure, evidence
- Recommend improvements

**Key State Updates**:
```python
{
    "feedback": "...",           # Detailed critique
    "critique_iteration": 2      # Iteration counter
}
```

**Model Recommendation**: Different provider (e.g., Claude Sonnet 4.5) for diverse perspective

**Why Different Provider**: Using a different model family provides a genuinely different evaluation perspective, catching issues that the same model family might overlook.

---

## State Management

### The EssayState Schema

The workflow operates on a comprehensive state object that flows through all nodes:

```python
class EssayState(TypedDict):
    # Core content
    messages: List[BaseMessage]
    topic: str
    thesis: str
    outline: str
    research_queries: List[str]
    research_results: List[dict]  # Changed to dict format with query context
    draft: str
    feedback: str
    editor_direction: str
    editor_decision: str

    # Control flow
    editing_iteration: int
    critique_iteration: int
    writing_iteration: int
    max_editing_iterations: int
    max_critique_iterations: int
    max_writing_iterations: int
    editing_complete: bool
    essay_complete: bool

    # Configuration
    max_essay_length: int
    max_queries: int                # NEW: Limit queries per research request
    max_results_per_query: int      # NEW: Limit results per query
    editor_model: dict
    researcher_model: dict
    writer_model: dict
    critic_model: dict

    # Tracking
    node_history: List[str]

    # Streaming UI (not shown in all examples)
    current_outline: str
    current_research_highlights: List[dict]
    current_draft: str
    current_feedback: str
```

### State Initialization

State is created using a helper function to ensure all fields are properly initialized:

```python
initial_state = create_initial_state(
    topic="Your topic here",
    editor_model=get_model_by_id("gpt-5.1"),
    researcher_model=get_model_by_id("gpt-5-nano"),
    writer_model=get_model_by_id("gpt-5-mini"),
    critic_model=get_model_by_id("claude-sonnet-4.5"),
    max_editing_iterations=5,
    max_critique_iterations=3,
    max_writing_iterations=2,
    max_essay_length=1500,
    max_queries=3,              # NEW: Limit research queries
    max_results_per_query=5     # NEW: Limit search results per query
)
```

### State Updates via Merge

Each node returns a dictionary with **only the fields it wants to update**. LangGraph automatically merges these updates into the existing state:

```python
# Node returns partial update
def editor_node(state: EssayState) -> dict:
    # ... processing ...
    return {
        "thesis": new_thesis,
        "outline": new_outline,
        "editing_iteration": state["editing_iteration"] + 1,
        "node_history": state["node_history"] + ["editor"]
    }

# LangGraph merges this into existing state
# Only specified fields are updated, others remain unchanged
```

---

## Workflow Phases

The workflow operates in two distinct phases with different routing logic.

### Phase 1: Initial Editing (Outline Development)

**Goal**: Develop a comprehensive thesis and outline with supporting research

**Flow**:
```
START → Editor → Researcher → Editor → Researcher → ... → Editor (editing_complete=True)
                                                              ↓
                                                           Writer
```

**Routing Logic**:
- Editor generates thesis, outline, and research queries
- Researcher executes queries and returns results
- Loop continues until editor sets `editing_complete=True`
- Constrained by `max_editing_iterations`

**State Indicators**:
- `draft == ""` (no draft yet)
- `editing_complete == False` → route to researcher
- `editing_complete == True` → route to writer

**Code Reference** (`graph/workflow.py:route_after_editor`):
```python
if state.get("draft", "") == "":
    # Initial editing: no draft yet
    if state.get("editing_complete", False):
        return "writer"  # Outline ready, start writing
    else:
        return "researcher"  # More research needed
```

---

### Phase 2: Critique & Revision Cycles

**Goal**: Iteratively improve draft quality through feedback and revision

**Flow**:
```
Writer → Critic → Editor → (decision)
                            ├─> Researcher (more research needed)
                            ├─> Writer (revise with direction)
                            └─> END (essay approved)
```

**Routing Logic**:
- Writer produces draft → always goes to Critic
- Critic evaluates draft → always goes to Editor
- Editor reviews feedback and decides:
  - **Research**: More evidence needed → Researcher → Editor → Writer
  - **Revise**: Clear direction for improvement → Writer
  - **Approve**: Quality sufficient → END

**State Indicators**:
- `draft != ""` (draft exists)
- `essay_complete == True` → END
- `editor_decision` determines routing

**Iteration Controls**:
- `max_critique_iterations`: Full cycles (Editor → Research/Write → Critique)
- `max_writing_iterations`: Revisions within a single critique cycle

**Code Reference** (`graph/workflow.py:route_after_editor`):
```python
# Critique review: draft exists
if state.get("essay_complete", False):
    return "complete"

decision = state.get("editor_decision", "revise")
if decision == "research":
    return "researcher"
elif decision == "pass_to_writer":
    return "writer"
else:  # "revise" or "approve"
    return "writer" if decision == "revise" else "complete"
```

---

## Routing Logic

### Deterministic Routes

Three nodes have deterministic (always the same) routing:

1. **Researcher → Editor** (always)
2. **Writer → Critic** (always)
3. **Critic → Editor** (always)

These are implemented as simple edges in the LangGraph:

```python
graph.add_edge("researcher", "editor")
graph.add_edge("writer", "critic")
graph.add_edge("critic", "editor")
```

---

### Conditional Route: Editor → ?

The editor has complex conditional routing based on workflow phase and state:

```python
def route_after_editor(state: EssayState) -> Literal["researcher", "writer", "complete"]:
    """
    Route after editor based on workflow phase and editorial decisions.

    Phase 1 (Initial Editing): draft is empty
        - editing_complete=False → researcher (continue outline development)
        - editing_complete=True → writer (start writing)

    Phase 2 (Critique & Revision): draft exists
        - essay_complete=True → complete (approved)
        - editor_decision="research" → researcher (commission research)
        - editor_decision="pass_to_writer" → writer (pass new research)
        - editor_decision="revise" → writer (revise with direction)
    """

    # PHASE 1: Initial editing (no draft yet)
    if state.get("draft", "") == "":
        if state.get("editing_complete", False):
            return "writer"  # Outline ready
        else:
            return "researcher"  # More outline research

    # PHASE 2: Critique review (draft exists)
    if state.get("essay_complete", False):
        return "complete"  # Approved

    # Route based on editorial decision
    decision = state.get("editor_decision", "revise")
    route_map = {
        "research": "researcher",
        "pass_to_writer": "writer",
        "revise": "writer",
        "approve": "complete"
    }
    return route_map.get(decision, "writer")
```

**Decision Tree**:
```
Editor completes
    │
    ├─ No draft yet?
    │   ├─ editing_complete? → writer
    │   └─ else → researcher
    │
    └─ Draft exists?
        ├─ essay_complete? → END
        └─ editor_decision?
            ├─ "research" → researcher
            ├─ "pass_to_writer" → writer
            ├─ "revise" → writer
            └─ "approve" → END
```

---

## Node History Tracking

### The Problem

Without tracking execution history, the editor could make illogical decisions:

**Problematic Scenario**:
1. Editor reviews critique: "We need more research on X"
2. Editor sets `editor_decision="research"`
3. Researcher executes queries, returns results
4. Editor receives results but **immediately approves essay** without passing research to writer

The research was commissioned but never used!

---

### The Solution: Node History

Track which nodes have executed to detect patterns and enforce logical flow:

```python
"node_history": ["editor", "researcher", "editor", "writer", "critic", "editor", "researcher"]
```

**Pattern Detection**:
```python
just_returned_from_research = (
    len(node_history) >= 2 and
    node_history[-1] == "researcher" and
    node_history[-2] == "editor" and
    state.get("draft", "") != ""  # Critique phase, not initial editing
)

if just_returned_from_research:
    # Editor commissioned research, must pass to writer
    return {
        "editor_decision": "pass_to_writer",
        "writing_iteration": 0,
        "node_history": updated_history
    }
```

**How It Works**:
- Each node appends its name to `node_history`
- Editor checks last two entries: `["editor", "researcher"]`
- If this pattern exists AND draft exists (critique phase), editor knows it just commissioned research
- Editor is **forced** to pass research to writer via `"pass_to_writer"` decision
- This prevents "research then immediately approve" logic bug

---

### Implementation in Each Node

Every node must append to history:

```python
def editor_node(state: EssayState) -> dict:
    node_history = state.get("node_history", [])
    updated_history = node_history + ["editor"]

    # ... node logic ...

    return {
        # ... other updates ...
        "node_history": updated_history
    }
```

---

## State Transitions

### Complete Workflow State Diagram

```
[START: Initial State]
   topic: "..."
   draft: ""
   editing_complete: False
   essay_complete: False
   editing_iteration: 0
        ↓
[EDITOR: Outline Development]
   thesis: "..."
   outline: "..."
   research_queries: ["q1", "q2"]
   editing_iteration: 1
   node_history: ["editor"]
        ↓
[RESEARCHER: Gather Information]
   research_results: ["result1"]
   node_history: ["editor", "researcher"]
        ↓
[EDITOR: Review Research]
   editing_iteration: 2
   research_queries: ["q3"]  (more research)
   node_history: ["editor", "researcher", "editor"]
        ↓
[RESEARCHER: Additional Research]
   research_results: ["result1", "result2"]
   node_history: [..., "researcher"]
        ↓
[EDITOR: Finalize Outline]
   editing_iteration: 3
   editing_complete: True
   node_history: [..., "editor"]
        ↓
[WRITER: Initial Draft]
   draft: "full essay text..."
   writing_iteration: 1
   node_history: [..., "writer"]
        ↓
[CRITIC: Evaluate Quality]
   feedback: "detailed critique..."
   critique_iteration: 1
   node_history: [..., "critic"]
        ↓
[EDITOR: Review Critique]
   editor_decision: "revise"
   editor_direction: "strengthen argument in section 2"
   node_history: [..., "editor"]
        ↓
[WRITER: Revise Draft]
   draft: "revised essay text..."
   writing_iteration: 2
   node_history: [..., "writer"]
        ↓
[CRITIC: Re-evaluate]
   feedback: "much improved, minor issues..."
   critique_iteration: 2
   node_history: [..., "critic"]
        ↓
[EDITOR: Final Review]
   editor_decision: "approve"
   essay_complete: True
   node_history: [..., "editor"]
        ↓
[END: Workflow Complete]
```

---

## Code Deep Dive

### Editor Node Implementation

Located in `graph/nodes.py`, the editor is the most complex node due to its dual role (outline development + critique review):

```python
def editor_node(state: EssayState) -> dict:
    """
    Editor orchestrates the entire workflow.

    Phase 1 (Initial Editing): Develops thesis and outline
    Phase 2 (Critique Review): Reviews feedback and directs revisions
    """

    # Track execution
    node_history = state.get("node_history", [])
    updated_history = node_history + ["editor"]

    # Get editor-specific model
    llm = get_llm(
        state["editor_model"]["provider"],
        state["editor_model"]["name"]
    )

    # =====================================================================
    # PHASE 1: INITIAL EDITING (Outline Development)
    # =====================================================================

    if state.get("draft", "") == "":
        editing_iteration = state.get("editing_iteration", 0) + 1

        # Check iteration limit
        if editing_iteration > state.get("max_editing_iterations", 3):
            # Force completion
            return {
                "editing_complete": True,
                "editing_iteration": editing_iteration,
                "node_history": updated_history
            }

        # Build prompt with topic and previous research
        prompt = EDITOR_INITIAL_PROMPT.format(
            topic=state["topic"],
            research="\n\n".join(state.get("research_results", [])),
            max_length=state.get("max_essay_length", 2000)
        )

        # Get editor response
        response = llm.invoke(prompt)

        # Parse response (thesis, outline, research queries, status)
        parsed = parse_editor_response(response.content)

        return {
            "thesis": parsed["thesis"],
            "outline": parsed["outline"],
            "research_queries": parsed["research_queries"],
            "editing_complete": parsed["editing_complete"],
            "editing_iteration": editing_iteration,
            "current_outline": parsed["outline"],
            "node_history": updated_history
        }

    # =====================================================================
    # PHASE 2: CRITIQUE REVIEW (Post-Draft Decision Making)
    # =====================================================================

    else:
        critique_iteration = state.get("critique_iteration", 0)

        # Check if we just returned from researcher (commissioned research)
        just_returned_from_research = (
            len(node_history) >= 2 and
            node_history[-1] == "researcher" and
            node_history[-2] == "editor"
        )

        if just_returned_from_research:
            # Must pass research to writer
            return {
                "editor_decision": "pass_to_writer",
                "editor_direction": "Incorporate the new research into your revision.",
                "writing_iteration": 0,  # Reset writing counter
                "node_history": updated_history
            }

        # Check iteration limit
        if critique_iteration >= state.get("max_critique_iterations", 3):
            # Force approval
            return {
                "essay_complete": True,
                "editor_decision": "approve",
                "node_history": updated_history
            }

        # Build prompt with draft, feedback, and research
        prompt = EDITOR_CRITIQUE_REVIEW_PROMPT.format(
            topic=state["topic"],
            outline=state.get("outline", ""),
            draft=state.get("draft", ""),
            feedback=state.get("feedback", ""),
            research="\n\n".join(state.get("research_results", []))
        )

        # Get editorial decision
        response = llm.invoke(prompt)
        parsed = parse_editor_decision(response.content)

        # Update iteration counters based on decision
        updates = {
            "editor_decision": parsed["decision"],
            "editor_direction": parsed["direction"],
            "node_history": updated_history
        }

        if parsed["decision"] == "research":
            updates["research_queries"] = parsed["research_queries"]
            updates["critique_iteration"] = critique_iteration + 1
        elif parsed["decision"] == "approve":
            updates["essay_complete"] = True

        return updates
```

**Key Points**:
1. Uses editor-specific model from state
2. Two completely different code paths for Phase 1 vs Phase 2
3. Node history check prevents research → approve bug
4. Iteration limits enforced with forced completion
5. Returns only changed fields (LangGraph merges into state)

---

### Researcher Node Implementation

Located in `graph/nodes.py`, the researcher is the simplest node—it just executes queries:

```python
def researcher_node(state: EssayState) -> dict:
    """Execute research queries and return consolidated results."""

    node_history = state.get("node_history", [])
    updated_history = node_history + ["researcher"]

    # Get researcher-specific model (typically cheapest)
    llm = get_llm(
        state["researcher_model"]["provider"],
        state["researcher_model"]["name"]
    )

    # Execute Tavily searches with configurable limits
    queries = state.get("research_queries", [])
    max_queries = state.get("max_queries", 3)
    max_results = state.get("max_results_per_query", 5)

    # Limit queries to max_queries
    queries = queries[:max_queries]

    research_tool = TavilySearchResults(max_results=max_results)

    all_results = []
    for query in queries:
        results = research_tool.invoke({"query": query})
        all_results.extend(results)

    # Synthesize research using LLM
    prompt = RESEARCHER_PROMPT.format(
        topic=state["topic"],
        queries=queries,
        raw_results=all_results
    )

    response = llm.invoke(prompt)
    summary = response.content

    # Append to research results (cumulative)
    updated_results = state.get("research_results", []) + [summary]

    # Prepare highlights for UI
    highlights = [
        {
            "query": query,
            "preview": summary[:200] + "..."
        }
        for query in queries
    ]

    return {
        "research_results": updated_results,
        "current_research_highlights": highlights,
        "node_history": updated_history
    }
```

**Why This Uses Most Tokens**:
- `all_results` contains full web page content from multiple sources
- LLM must process ~50,000 tokens of raw web content
- Synthesizes into concise summary
- **Cost optimization critical**: Use cheapest capable model
- **Configuration helps**: Reduce `max_queries` and `max_results_per_query` to lower token usage and costs

---

### Writer Node Implementation

Located in `graph/nodes.py`, the writer handles both initial drafts and revisions:

```python
def writer_node(state: EssayState) -> dict:
    """Generate or revise essay draft."""

    node_history = state.get("node_history", [])
    updated_history = node_history + ["writer"]
    writing_iteration = state.get("writing_iteration", 0) + 1

    # Get writer-specific model
    llm = get_llm(
        state["writer_model"]["provider"],
        state["writer_model"]["name"]
    )

    # Check iteration limit
    if writing_iteration > state.get("max_writing_iterations", 3):
        # Return existing draft
        return {
            "writing_iteration": writing_iteration,
            "node_history": updated_history
        }

    # Determine if initial draft or revision
    is_revision = state.get("draft", "") != ""

    if is_revision:
        # Revision: include feedback and editor direction
        prompt = WRITER_REVISION_PROMPT.format(
            topic=state["topic"],
            outline=state.get("outline", ""),
            research="\n\n".join(state.get("research_results", [])),
            previous_draft=state["draft"],
            feedback=state.get("feedback", ""),
            editor_direction=state.get("editor_direction", ""),
            max_length=state.get("max_essay_length", 2000)
        )
    else:
        # Initial draft: from outline and research
        prompt = WRITER_INITIAL_PROMPT.format(
            topic=state["topic"],
            outline=state["outline"],
            research="\n\n".join(state.get("research_results", [])),
            max_length=state.get("max_essay_length", 2000)
        )

    # Generate draft
    response = llm.invoke(prompt)
    new_draft = response.content

    return {
        "draft": new_draft,
        "writing_iteration": writing_iteration,
        "current_draft": new_draft,
        "node_history": updated_history
    }
```

**Key Points**:
1. Increments `writing_iteration` on each invocation
2. Different prompts for initial vs revision
3. Revisions include feedback AND editor direction (specific guidance)
4. Updates `current_draft` for UI streaming

---

### Critic Node Implementation

Located in `graph/nodes.py`, the critic evaluates and provides feedback:

```python
def critic_node(state: EssayState) -> dict:
    """Evaluate draft and provide feedback."""

    node_history = state.get("node_history", [])
    updated_history = node_history + ["critic"]
    critique_iteration = state.get("critique_iteration", 0) + 1

    # Get critic-specific model (ideally different provider)
    llm = get_llm(
        state["critic_model"]["provider"],
        state["critic_model"]["name"]
    )

    # Build evaluation prompt
    prompt = CRITIC_PROMPT.format(
        topic=state["topic"],
        outline=state.get("outline", ""),
        draft=state["draft"],
        target_length=state.get("max_essay_length", 2000)
    )

    # Get critique
    response = llm.invoke(prompt)
    feedback = response.content

    return {
        "feedback": feedback,
        "critique_iteration": critique_iteration,
        "current_feedback": feedback,
        "node_history": updated_history
    }
```

**Why Different Provider**:
Using a different model family (e.g., Claude when writer uses GPT) provides genuinely different evaluation perspective, catching issues the original model might miss.

---

## Iteration Controls

Three separate iteration limits control workflow loops:

### 1. Max Editing Iterations

**Controls**: Research/outline refinement cycles (Phase 1)

**Loop**: Editor ↔ Researcher

**Limit Behavior**: When `editing_iteration > max_editing_iterations`, editor is **forced** to set `editing_complete=True`, advancing to writing phase.

**Range**: 2-12 iterations
**Default**: 5 iterations
**Typical Values**:
- Quick essays: 2-3
- Standard essays: 4-6
- Research papers: 6-8

---

### 2. Max Critique Iterations

**Controls**: Full critique cycles (Phase 2)

**Loop**: Editor → Research/Writing → Critique

**Limit Behavior**: When `critique_iteration >= max_critique_iterations`, editor is **forced** to approve essay (set `essay_complete=True`).

**Range**: 1-8 cycles
**Default**: 3 cycles
**Typical Values**:
- Quick essays: 2
- Standard essays: 3-4
- Research papers: 4-5

---

### 3. Max Writing Iterations

**Controls**: Revisions within a single critique cycle

**Loop**: Writer revisions before returning to critic

**Limit Behavior**: When `writing_iteration > max_writing_iterations`, writer returns existing draft unchanged.

**Range**: 2-8 iterations
**Default**: 3 iterations
**Typical Values**:
- Quick essays: 2-3
- Standard essays: 3-4
- Research papers: 4-5

**Note**: This counter is **reset to 0** when editor commissions research (`editor_decision="research"` or `"pass_to_writer"`), allowing fresh revisions after new information is available.

---

### 4. Research Configuration (NEW)

**Max Queries per Research Request**:
- **Controls**: Number of search queries the editor can request at once
- **Range**: 1-5 queries
- **Default**: 3 queries
- **Impact**: More queries = broader research but higher token usage

**Max Results per Query**:
- **Controls**: Number of search results retrieved per query
- **Range**: 2-10 results
- **Default**: 5 results
- **Impact**: More results = deeper research but significantly higher token usage

**Cost Optimization Strategy**:
```python
# Low-cost setup (minimal research)
max_queries=2
max_results_per_query=3

# Balanced setup (recommended)
max_queries=3
max_results_per_query=5

# Comprehensive research (high cost)
max_queries=4
max_results_per_query=8
```

---

## Advanced Patterns

### Pattern 1: Research → Pass to Writer

**Scenario**: After critique, editor decides more research is needed.

**Execution Flow**:
```
Critic → Editor (review feedback)
              ↓
         decision="research"
         research_queries=[...]
              ↓
         Researcher (execute queries)
              ↓
         Editor (node history check detects ["editor", "researcher"])
              ↓
         decision="pass_to_writer"
         writing_iteration=0  (reset counter)
              ↓
         Writer (incorporate new research)
```

**Key Code** (editor node):
```python
if just_returned_from_research:
    return {
        "editor_decision": "pass_to_writer",
        "writing_iteration": 0,  # Allow fresh revisions with new info
        "node_history": updated_history
    }
```

---

### Pattern 2: Forced Completion

**Scenario**: Iteration limit reached but workflow still wants to continue.

**Behavior**: Editor overrides its own decision to force completion.

**Code** (editing phase):
```python
if editing_iteration > max_editing_iterations:
    return {
        "editing_complete": True,  # Force advance
        "editing_iteration": editing_iteration,
        "node_history": updated_history
    }
```

**Code** (critique phase):
```python
if critique_iteration >= max_critique_iterations:
    return {
        "essay_complete": True,  # Force approval
        "editor_decision": "approve",
        "node_history": updated_history
    }
```

This prevents infinite loops while still allowing the editor to naturally complete earlier if quality is sufficient.

---

### Pattern 3: Per-Agent Model Optimization

**Strategy**: Assign models based on task complexity and token usage.

**Example Configuration**:
```python
initial_state = create_initial_state(
    topic=topic,
    editor_model=get_model_by_id("gpt-5.1"),          # Most capable
    researcher_model=get_model_by_id("gpt-5-nano"),   # Cheapest
    writer_model=get_model_by_id("gpt-5-mini"),       # Balanced
    critic_model=get_model_by_id("claude-sonnet-4.5"), # Different provider
    max_queries=3,                                     # Moderate research
    max_results_per_query=5                            # Balanced depth
)
```

**Rationale**:
- **Editor**: Strategic decisions require best reasoning → premium model
- **Researcher**: Summarization task with high token volume → cheapest model + limited queries
- **Writer**: Quality prose but not deep reasoning → mid-tier model
- **Critic**: Different perspective valuable → alternate provider

**Cost Impact**:
- Model selection: Can reduce costs by 10-20x compared to using premium model for everything
- Research configuration: Reducing `max_queries` from 5 to 3 and `max_results_per_query` from 10 to 5 can cut research costs in half
- Combined: Total cost savings of 15-30x with minimal quality impact

---

## Monitoring & Debugging

### LangSmith Integration

Every node execution is traced in LangSmith when `LANGCHAIN_TRACING_V2=true`.

**What You Can See**:
- Complete state evolution through workflow
- Token usage per node (identify cost bottlenecks)
- Model invocations with full prompts and responses
- Routing decisions at each conditional edge
- Error traces with full context

**Example Use Case**: Identify that researcher uses 50K tokens per cycle → switch to cheaper model.

---

### Node History Debugging

Print node history at any point to understand execution flow:

```python
print(f"Node History: {state['node_history']}")
# Output: ['editor', 'researcher', 'editor', 'writer', 'critic', 'editor', 'researcher', 'editor']
```

**Pattern Recognition**:
- `["editor", "researcher", "editor"]`: Editor requested research, reviewed it
- `["writer", "critic", "editor"]`: Draft evaluated, editor deciding next step
- `["editor", "researcher", "editor", "writer"]`: Research passed to writer

---

### Debug Script

Use `debug_workflow.py` to step through execution without UI:

```bash
python debug_workflow.py
```

Set breakpoints in your IDE to inspect state at each node completion.

---

## Summary

This agentic workflow demonstrates:

1. **Intelligent Orchestration**: Editor makes adaptive decisions, not just following fixed paths
2. **Stateful Coordination**: Comprehensive state tracking enables complex routing logic
3. **Pattern Detection**: Node history prevents illogical decision sequences
4. **Iterative Refinement**: Multiple feedback loops drive quality improvement
5. **Cost Optimization**: Per-agent models balance quality and cost
6. **Production Patterns**: Robust error handling, iteration limits, monitoring

The result is a **self-improving system** that produces high-quality essays through autonomous agent collaboration, demonstrating key concepts in agentic AI programming.

---

**Next Steps**:
- Extend with additional agents (fact-checker, citation formatter)
- Implement confidence-based routing (route to different agents based on uncertainty)
- Add human-in-the-loop approval gates for critical decisions
- Experiment with different model combinations for your use case

---

*This workflow architecture is inspired by concepts from the Zero To Mastery (ZTM) LLM course and extended with production-grade agentic patterns.*
